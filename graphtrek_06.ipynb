{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "graphtrek_06.ipynb",
      "provenance": [],
      "mount_file_id": "1Sb5kXvEjzgyWmvJhmOQKkXuEq-wjsDpm",
      "authorship_tag": "ABX9TyMjGmNxSxGCxHPTTgy+KGrE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graphtrek/stockforecast/blob/main/graphtrek_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FresKqY08M_Q"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MAE\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow import keras\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import os, time\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "from pandas.tseries.offsets import CustomBusinessDay\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPIvUMpvBkwk"
      },
      "source": [
        "ticker = \"SPY\"\n",
        "look_back = 5 #  number of past days we want to use to predict 1 day in the future.\n",
        "max_data_size = 730 # ~2 years\n",
        "split_percent = 0.90 # use 90% of the data  for train\n",
        "print_level = 'DEBUG'\n",
        "model_file_path = '/content/drive/MyDrive/models/'+ticker+'_06.h5'\n",
        "nr_of_features = 2\n",
        "look_forward = 1\n",
        "epochs =150\n",
        "learning_rate=0.0001\n",
        "num_prediction = look_back * 2\n",
        "use_values = False\n",
        "#np.random.seed(42)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Yd1C1a2BoAB",
        "outputId": "0496f907-eddf-4d6d-8c60-23a8a2aea981"
      },
      "source": [
        "url = 'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol='+ticker+'&outputsize=full&apikey=3F4URDEKOPLFH25T'\n",
        "print(url)\n",
        "stock_api_response = pd.read_json(url)\n",
        "stock_api_data = stock_api_response['Time Series (Daily)']\n",
        "#print(stock_api_data)\n",
        "stock_api_data = stock_api_data.drop(index=['1. Information','2. Symbol','3. Last Refreshed','4. Output Size','5. Time Zone']);\n",
        "\n",
        "data = []\n",
        "lstm_data = []\n",
        "split_coefficient = 1\n",
        "\n",
        "for key, value in stock_api_data.items():\n",
        "  date = key\n",
        "  open = float(value.get('1. open'))\n",
        "  high = float(value.get('2. high'))\n",
        "  low = float(value.get('3. low'))\n",
        "  close = float(value.get('4. close'))\n",
        "  adjusted_close = float(value.get('5. adjusted close'))\n",
        "  volume = int(value.get('6. volume'))\n",
        "  divident = float(value.get('7. dividend amount'))\n",
        "\n",
        "  if float(value.get('8. split coefficient')) > 1:\n",
        "    split_coefficient = float(value.get('8. split coefficient'))\n",
        "    \n",
        "  open = open / split_coefficient\n",
        "  high = high /split_coefficient\n",
        "  close = close / split_coefficient\n",
        "  low = low / split_coefficient\n",
        "  \n",
        "  # 4 features\n",
        "  lstm_data.append([\n",
        "    close,\n",
        "    open\n",
        "  ])\n",
        "\n",
        "  data.append([\n",
        "      date,\n",
        "      close,\n",
        "      volume,\n",
        "      high,\n",
        "      low,\n",
        "      open,\n",
        "      divident\n",
        "      ])\n",
        "\n",
        "last_date =  str(data[0][0])\n",
        "\n",
        "if max_data_size < len(data):\n",
        "  data = np.flip(data[:max_data_size],axis=0)\n",
        "  lstm_data = np.flip(lstm_data[:max_data_size],axis=0)\n",
        "else:\n",
        "  data = np.flip(data,axis=0)\n",
        "  lstm_data = np.flip(lstm_data,axis=0)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=SPY&outputsize=full&apikey=3F4URDEKOPLFH25T\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHeShfGapFbn"
      },
      "source": [
        "# Normalised values [0,max_price] as integer and add to the features\n",
        "if use_values is True:\n",
        "  values = data[:,2].astype(int)\n",
        "  max_price = np.amax(lstm_data)\n",
        "\n",
        "  values_scaled = (max_price*(values - np.min(values))/np.ptp(values)).astype(float)\n",
        "  lstm_data = np.append(lstm_data, np.expand_dims(values_scaled, axis=1), axis=1)\n",
        "\n",
        "  if print_level == 'TRACE':\n",
        "    print('max_price:',max_price)\n",
        "    print('values:',values[:5])\n",
        "    print('values_scaled',values_scaled[:5])\n",
        "    print('lstm_data:',lstm_data[:5])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PtRy5TxC8BG"
      },
      "source": [
        "if print_level == 'TRACE':\n",
        "  np.set_printoptions(formatter={'float': '{: 0.2f}'.format})\n",
        "  print('data ===>', 'size:',len(data), 'max_data_size:', max_data_size, 'shape:', data.shape)\n",
        "  print(data[:look_back])\n",
        "  print('lstm_data ===>', 'size:',len(lstm_data), 'max_data_size:', max_data_size, 'shape:', lstm_data.shape)\n",
        "  print(lstm_data[:look_back])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BI3hlg-Ejtr"
      },
      "source": [
        "df = pd.DataFrame(data,columns=['Date','Close','Volume','High','Low','Open','Divident'])\n",
        "\n",
        "df['50MA'] = df['Close'].rolling(50).mean()\n",
        "df['100MA'] = df['Close'].rolling(100).mean()\n",
        "df['200MA'] = df['Close'].rolling(200).mean()\n",
        "\n",
        "split = int(split_percent*len(data))\n",
        "if len(data) - split < look_back:\n",
        "  split = look_back\n",
        "split\n",
        "#df.head()\n",
        "\n",
        "df_train = df.iloc[:split]\n",
        "df_test = df.iloc[split:]\n",
        "\n",
        "lstm_train_data = lstm_data[:split]\n",
        "lstm_test_data = lstm_data[split:]"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4g3hz7tE5_l"
      },
      "source": [
        "if print_level == 'TRACE':\n",
        "  print('df_test ===>', 'size:',len(df_test),'shape:', df_test.shape)\n",
        "  print(df_test.head())\n",
        "\n",
        "  print('df_train ===>', 'size:',len(df_train),'shape:', df_train.shape)\n",
        "  print(df_train.head())\n",
        "\n",
        "  print('lstm_test_data ===>', 'size:',len(lstm_test_data), 'shape:', lstm_test_data.shape)\n",
        "  print(lstm_test_data[:2*(look_back + 1)])\n",
        "\n",
        "  print('lstm_train_data ===>', 'size:',len(lstm_train_data), 'shape:', lstm_train_data.shape)\n",
        "  print(lstm_train_data[:2*(look_back + 1)])"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsqqTXD0tYAi",
        "outputId": "51070fb0-09af-482b-ec94-993e02be55f1"
      },
      "source": [
        "np.set_printoptions(formatter={'float': '{: 0.12f}'.format})\n",
        "\n",
        "print(lstm_data[-1])\n",
        "min_data = np.amin(lstm_data)\n",
        "mean_data = np.mean(lstm_data)\n",
        "max_data = np.amax(lstm_data) * 1.2\n",
        "\n",
        "print('mean:', mean_data, 'max:', max_data, 'min:',0)\n",
        "#scaler_data = np.append(lstm_data,[\n",
        "#    max_data,\n",
        "#    max_data,\n",
        "#    max_data,\n",
        "#    max_data,\n",
        "#    max_data\n",
        "#  ])\n",
        "\n",
        "#print(scaler_data[-2])\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler = scaler.fit(lstm_data)\n",
        "\n",
        "lstm_train_data_scaled = scaler.transform(lstm_train_data)\n",
        "if print_level == 'DEBUG':\n",
        "  print(lstm_train_data_scaled[:look_back+1])\n",
        "  print(lstm_train_data_scaled.shape)\n",
        "\n",
        "lstm_test_data_scaled = scaler.transform(lstm_test_data)\n",
        "if print_level == 'DEBUG':\n",
        "  print(lstm_test_data_scaled[:look_back+1])\n",
        "  print(lstm_test_data_scaled.shape)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 459.250000000000  455.870000000000]\n",
            "mean: 337.01346226027397 max: 551.1 min: 0\n",
            "[[ 0.171900126957  0.180210471158]\n",
            " [ 0.174016081253  0.153617745950]\n",
            " [ 0.174269995768  0.172350552378]\n",
            " [ 0.179898434194  0.171520894284]\n",
            " [ 0.179517562421  0.167372603816]\n",
            " [ 0.158781210326  0.151827431117]]\n",
            "(657, 2)\n",
            "[[ 0.880702496826  0.862364088904]\n",
            " [ 0.895471857808  0.891445788394]\n",
            " [ 0.899322894625  0.901925680101]\n",
            " [ 0.918281845112  0.914064887996]\n",
            " [ 0.922852306390  0.921881140562]\n",
            " [ 0.914346170123  0.924501113488]]\n",
            "(73, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRWuDtOxHmB8"
      },
      "source": [
        "train_generator = TimeseriesGenerator(lstm_train_data_scaled, lstm_train_data_scaled, length=look_back, batch_size=look_forward)\n",
        "\n",
        "if print_level == 'TRACE':\n",
        "  print('Samples: %d' % len(train_generator))\n",
        "  # print each sample\n",
        "  for i in range(2):\n",
        "    x, y = train_generator[i]\n",
        "    print('%s => %s' % (x, y))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FDhRypzKvVX",
        "outputId": "a9148be9-3a9d-4ff2-b316-40fb0806ffc1"
      },
      "source": [
        "model = None\n",
        "try:\n",
        "  model = keras.models.load_model(model_file_path)\n",
        "  modified = os.path.getmtime(model_file_path)\n",
        "\n",
        "  print(pd.to_datetime(last_date).date() - datetime.fromtimestamp(modified).date() )\n",
        "  print('Loaded', model_file_path , ' model train date:',datetime.fromtimestamp(modified).date() , 'last date:', last_date)\n",
        "except:\n",
        "  model = None\n",
        "  print('Model ' + model_file_path + ' does not exists.')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model /content/drive/MyDrive/models/SPY_06.h5 does not exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMqc0Uavxn0T",
        "outputId": "cc05e1ea-5a67-4266-97f4-b37b2766b4e5"
      },
      "source": [
        "if model is None:\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(units=50, activation='tanh', input_shape=(look_back, nr_of_features), return_sequences=True))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(units=50, return_sequences=True, activation=\"tanh\"))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(units=50, return_sequences=True, activation=\"tanh\"))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(units=50, activation=\"tanh\"))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(nr_of_features))\n",
        "\n",
        "  model.compile(\n",
        "      loss=MAE,\n",
        "      optimizer=Adam(learning_rate=learning_rate),\n",
        "      metrics=[\"mae\"]\n",
        "  )\n",
        "    \n",
        "  #model.summary()\n",
        "  print('Model ' + model_file_path + ' compiled.')\n",
        "\n",
        "  modelo = model.fit(train_generator, epochs=epochs, verbose=0)\n",
        "  \n",
        "  model.save(model_file_path)\n",
        "  print('Saved model ' + model_file_path)\n",
        "\n",
        "  if print_level == 'DEBUG':\n",
        "    plt.plot(modelo.history['loss'])\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model /content/drive/MyDrive/models/SPY_06.h5 compiled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zom6mqtPJunz"
      },
      "source": [
        "test_generator = TimeseriesGenerator(lstm_test_data_scaled, lstm_test_data_scaled, length=look_back, batch_size=look_forward)\n",
        "\n",
        "print('Samples: %d' % len(test_generator))\n",
        "# print each sample\n",
        "if print_level == 'TRACE':\n",
        "  for i in range(2):\n",
        "    x, y = test_generator[i]\n",
        "    print('%s => %s' % (x, y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJmwCRQs5Wed"
      },
      "source": [
        "#Make prediction\n",
        "prediction_scaled = model.predict(test_generator)\n",
        "\n",
        "if print_level == 'DEBUG':\n",
        "  np.set_printoptions(formatter={'float': '{: 0.12f}'.format})\n",
        "  print(prediction_scaled[:5])\n",
        "\n",
        "prediction = scaler.inverse_transform(prediction_scaled)\n",
        "pred_prices = prediction[:,0]\n",
        "\n",
        "if print_level == 'DEBUG':\n",
        "  np.set_printoptions(formatter={'float': '{: 0.2f}'.format})\n",
        "  print('prediction shape:',prediction.shape)\n",
        "\n",
        "  for x in range(look_back + 1):\n",
        "    print('pred:',prediction[x], 'test:', lstm_test_data[x])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWb02fuJtYE8"
      },
      "source": [
        "def predict(num_prediction, model):\n",
        "    prediction_list = lstm_test_data_scaled[-look_back:]\n",
        "    print(prediction_list)\n",
        "    for _ in range(num_prediction):\n",
        "        x = prediction_list[-look_back:]\n",
        "        #print(x)\n",
        "        x = x.reshape((1, look_back, nr_of_features))\n",
        "        out = model.predict(x)\n",
        "        #print(x,out) \n",
        "        prediction_list = np.append(prediction_list, out, axis=0)\n",
        "    prediction_list = prediction_list[look_back-1:]\n",
        "        \n",
        "    return prediction_list\n",
        "    \n",
        "def predict_dates(last_date,num_prediction):\n",
        "    us_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n",
        "    prediction_dates = pd.date_range(last_date, periods=num_prediction+1,freq=us_bd).tolist()\n",
        "    return prediction_dates\n",
        "\n",
        "forecast_scaled = predict(num_prediction, model)\n",
        "#print(forecast_scaled)\n",
        "forecast_dates = predict_dates(df['Date'].values[-1],num_prediction)\n",
        "\n",
        "forecast = scaler.inverse_transform(forecast_scaled.reshape((-1,nr_of_features)))\n",
        "forecast_prices = forecast[0::nr_of_features].reshape((-1))\n",
        "#forecast\n",
        "forecast_prices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euPxjxvHH91z"
      },
      "source": [
        "trace1 = go.Candlestick(\n",
        "    x = df_train['Date'],\n",
        "    open = df_train['Open'],\n",
        "    high = df_train['High'],\n",
        "    low = df_train['Low'],\n",
        "    close = df_train['Close'],\n",
        "    name = 'Train'\n",
        ")\n",
        "\n",
        "trace2 = go.Candlestick(\n",
        "    x = df_test['Date'],\n",
        "    open = df_test['Open'],\n",
        "    high = df_test['High'],\n",
        "    low = df_test['Low'],\n",
        "    close = df_test['Close'],\n",
        "    increasing={'line': {'color': 'lightblue'}},\n",
        "    decreasing={'line': {'color': 'purple'}},    \n",
        "    name ='Test'\n",
        ")\n",
        "\n",
        "trace3 = go.Scatter(\n",
        "    x = df_test['Date'],\n",
        "    y = pred_prices,\n",
        "    name ='Test'\n",
        ")\n",
        "\n",
        "trace4 = go.Scatter(\n",
        "    x = forecast_dates,\n",
        "    y = forecast_prices,\n",
        "    name ='Forecast'\n",
        ")\n",
        "\n",
        "trace5 = go.Scatter(\n",
        "    x = df['Date'],\n",
        "    y = df['50MA'],\n",
        "    mode='lines',\n",
        "    name ='50MA'\n",
        ")\n",
        "\n",
        "trace6 = go.Scatter(\n",
        "    x = df['Date'],\n",
        "    y = df['100MA'],\n",
        "    mode='lines',\n",
        "    name ='100MA'\n",
        ")\n",
        "\n",
        "trace7 = go.Scatter(\n",
        "    x = df['Date'],\n",
        "    y = df['200MA'],\n",
        "    mode='lines',\n",
        "    name ='200MA'\n",
        ")\n",
        "\n",
        "#'Date','Close','Volume','High','Low','Open','Divident'\n",
        "open_price =  str(data[-1][5])\n",
        "low_price =  str(data[-1][3])\n",
        "high_price =  str(data[-1][4])\n",
        "close_price =  str(data[-1][1])\n",
        "\n",
        "layout = go.Layout(\n",
        "    title = ticker + ' Date:' + last_date + ' Open:' + open_price + ' High:' + high_price + ' Low:' + low_price + ' Close:' + close_price,\n",
        "    xaxis = {'title' : \"Dates\"},\n",
        "    yaxis = {'title' : \"Close Price ($)\"},\n",
        "    height = 450\n",
        ")\n",
        "\n",
        "fig = go.Figure(data=[trace1, trace2, trace3, trace4, trace5, trace6, trace7], layout=layout)\n",
        "\n",
        "fig.update_yaxes(showspikes=True, spikemode='across', spikesnap='cursor',spikedash='dash')\n",
        "fig.update_xaxes(showspikes=True, spikemode='across', spikesnap='cursor', spikedash='dash')\n",
        "fig.update_layout(xaxis_rangeslider_visible=False)\n",
        "config = dict({'scrollZoom': True})\n",
        "fig.show(config=config)\n",
        "\n",
        "fig.write_html('/content/drive/MyDrive/models/charts/'+ticker+ '_06_' + last_date + '_forecast.html')\n",
        "\n",
        "fig1 = px.bar(\n",
        "    x=df['Date'], \n",
        "    y=df['Volume'], \n",
        "    height=250, \n",
        "    labels={\"x\": \"Dates\",\"y\": \"Volume\"},\n",
        "    title=ticker)\n",
        "fig1.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}